---
title: "Section 5.2 Confidence Intervals and P-values Using Normal Distributions"
author: "E. Nordmoe"
date: "Math 260"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE,warning=FALSE)
```


```{r, echo=F,message=FALSE}
library(mosaic)
library(Lock5Data)
```


# Outline

* Central Limit Theorem  
* Confidence interval using a normal distribution  
* Hypothesis test using a normal distribution  

---

# Central Limit Theorem  

> For random samples with a *sufficiently large* sample size, the distribution of sample statistics for a mean or a proportion is approximately normal.

---

# Central Limit Theorem  

* The central limit theorem holds for **any** original distribution, although "sufficiently large sample size" varies.

    + The more skewed the original distribution is, the larger $n$ has to be for the CLT to work.

* For quantitative variables that are not very skewed, $n\ge 30$ is usually sufficient.

* For categorical variables, counts of at least 10 within each category are usually sufficient.

???

 ## Example: Hearing Loss in Teenagers   

New York Times [report](http://www.nytimes.com/2010/08/24/health/research/24child.html?_r=0)  on a study of hearing loss in teenagers:

What proportion of Americans aged 12 to 19 have some hearing loss?  Give a 95% CI. 

Use bootstrap to get percentile interval; then use SE to get 95% interval. Extend to other levels using the basic form getting the critical value from the standard normal

---

# Confidence Intervals Using the Standard Normal  

If a statistic is normally distributed, we find a P% confidence interval for the parameter using

$$\text{statistic} \pm z^* \text{SE} $$  

where $z^*$ is the *critical value* with area P% between $-z^*$ and $z^*$ in the standard normal distribution.   

---

# P-values

If the randomization distribution is normal:  

> To calculate a $p$-value, we just need to find the area in the appropriate tail(s) beyond the observed statistic of the distribution   

$$
N(\text{null value}, \text{SE})
$$

---

# Standardized Test Statistic  


> The standardized *test statistic* is the number of standard errors a test statistic is away from the hypothesized null value:  

$$
z=\frac{\text{Sample statistic}-\text{Null parameter}}{\text{SE}}
$$

* Calculating the number of standard errors a statistic is from the null value allows us to assess *extremity* on a common scale.  

---

# P-value Using the Standard Normal  


If a statistic is normally distributed under $H_0$, the **p-value** is the probability  a standard normal is at or beyond  

$$
z=\frac{\text{Sample statistic}-\text{Null parameter}}{\text{SE}}
$$

--
* One-tailed $p$-values  
    + $p=P(Z\ge z)$ or $p=P(Z\le z)$

* Two-tailed $p$-value  
    + Double the one-tailed value  
  
where $P(Z\ge z)$ means the probability *to the right* of the observed test statistic $z$ under the standard normal density represented by $Z$.  

---

# Summary: General Formulas

### Confidence interval

$$
\text{Sample Statistic}\pm z^* \text{SE}
$$

--

### Hypothesis test statistic

$$
\frac{\text{Sample statistic}-\text{Null parameter}}{\text{SE}}
$$

---

# Looking ahead

* For now, the $\text{SE}$ comes from **resampling** methods (randomization or bootstrap).

* Beginning next class, we use classical model-based **formulas** to compute $\text{SE}$.  
