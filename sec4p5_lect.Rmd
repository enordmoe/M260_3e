---
title: "4.5 Confidence Intervals and Hypothesis Tests"
author: "E. Nordmoe"
date: "Math 260"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE,warning=FALSE)
```


```{r, echo=F,message=FALSE}
library(mosaic)
library(Lock5Data)
```


# Bootstrap and Randomization Distributions

 Bootstrap Distributions | Randomization Distributions  
:----------------------------:|:------------------------------:
Our best guess at the distribution of sample statistics | Our best guess at the distribution of sample statistics, if $H_0$ were true   
Centered around the observed sample statistic | Centered around the null hypothesized value  
Simulate sampling from the population by resampling from the original sample |   Simulate samples assuming $H_0$ were true  


---
class:center, middle, inverse

# Key difference  

### Randomization distribution assumes $H_0$ true, <br> while a bootstrap distribution does *not*

---

# Intervals and Tests


* If a 95% CI **contains** the parameter in $H_0$, then a two-tailed test should **not reject** $H_0$ at the 5% significance level.

* If a 95% CI **misses** the parameter in $H_0$, then a two-tailed test should **reject** $H_0$ at the 5% significance level.

---

# Intervals and Tests

A confidence interval represents the **range of plausible values** for the population parameter

* If the null hypothesized value is **not within** the CI, it is **not a plausible** value and **should be rejected.**  

* If the null hypothesized value is **within** the CI, it **is a plausible** value and **should not be rejected.**  

---

# Intervals and Tests

* Confidence intervals give you a range of plausible values for the parameter.

* A significance test describes the plausibility of one specific value of the parameter (through the $p$-value).

---

## Statistical vs Practical Significance

*  With small sample sizes, even large differences or effects may not be significant.

*  With large sample sizes, even a very small difference or effect can be significant.

*  A statistically significant result is not always practically significant, especially with large sample sizes.

---

# Multiple Testing

*  When multiple hypothesis tests are conducted, the chance that at least one test incorrectly rejects a true null hypothesis increases with the number of tests.  

*  If the null hypotheses are all true, $\alpha$ of the tests will yield statistically significant results just by random chance. 

---

# Multiple Testing xkcd Style

http://imgs.xkcd.com/comics/significant.png

---

# Publication Bias

*  **Publication bias** refers to the fact that usually only the significant results get published.  

*  The one study that turns out significant gets published, and no one knows about all the insignificant results.  

*  This combined with the problem of multiple comparisons, can yield very misleading results.  

---

# Summary

*  If a null hypothesized value lies **inside** a 95% CI, a two-tailed test using $\alpha=.05$ would **not reject** $H_0$.  

*  If a null hypothesized value lies **outside** a 95% CI, a two-tailed test using $\alpha=.05$ would **reject** $H_0$.  

* **Statistical significance** is not at all the same as **practical significance**. 

* Using $\alpha=.05$, about 5% of all hypothesis tests will lead to rejecting the null, even if all the null hypotheses are true.  

